{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "data_dir = './data'\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random as rand\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plot\n",
    "#from utils.data_utils import *\n",
    "from utils.vis_utils import *\n",
    "from utils.layer_utils import *\n",
    "from utils.print_utils import *\n",
    "from utils.resnet import *\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "#import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "#tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Ready for the Data\n",
    "def _process_and_save(normalize, features, labels, filename):\n",
    "    \"\"\"\n",
    "    Preprocess data and save it to file\n",
    "    \"\"\"\n",
    "    features = normalize(features)\n",
    "    labels = labels\n",
    "    f = open(filename, 'wb')\n",
    "    pickle.dump((features, labels), f)\n",
    "    f.close()\n",
    "\n",
    "def preprocess_and_data(loadFiles, normalize):\n",
    "    \"\"\"\n",
    "    Preprocess Training and Validation Data\n",
    "    \"\"\"\n",
    "    n_batches = 5\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "    features, labels, test_feature, test_labels, val_features, val_labels = loadFiles(True, sample_rate = 0.5)\n",
    "    \n",
    "    _process_and_save(normalize, features, labels, 'preprocess_batch_tranning.p')\n",
    "    _process_and_save(normalize, test_feature, test_labels, 'preprocess_batch_testing.p')\n",
    "    _process_and_save(normalize, val_features, val_labels, 'preprocess_batch_val.p')\n",
    "\n",
    "    \n",
    "\n",
    "#cap2 = cv2.VideoCapture('Pig_Identification_Qualification_Train/train/1.mp4')\n",
    "def center_crop(img):\n",
    "    shape = img.shape\n",
    "    h = shape[0]\n",
    "    w = shape[1]\n",
    "    c = shape[2]\n",
    "    if w > h:\n",
    "        lbound = int((w - h) / 2)\n",
    "        rbound = w - lbound\n",
    "        return img[:, lbound:rbound, :]\n",
    "    else:\n",
    "        tbound = int((h - w) / 2)\n",
    "        bbound = h - tbound\n",
    "        return img[tbound:bbound, :, :]\n",
    "\n",
    "\n",
    "def LoadVideo(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    nbFrames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    images = []\n",
    "\n",
    "    for i in range(nbFrames):\n",
    "        ret, frame = cap.read()\n",
    "        #print(ret)\n",
    "        if ret:\n",
    "            images.append(center_crop(frame[:,:,::-1])) #convert data from BGR color space to RGB\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return images, nbFrames\n",
    "    \n",
    "\n",
    "#imgs, frameCount = LoadVideo('Pig_Identification_Qualification_Train/train/1.mp4')\n",
    "#TestImg  = imgs[rand.randint(0, frameCount)]\n",
    "#pImg = Image.fromarray(TestImg, mode='RGB')\n",
    "#pImg.show()\n",
    "\n",
    "#while(1):\n",
    "#    ret, frame = cap.read()\n",
    "#    cv2.imshow('pigtest', frame)\n",
    "#    if cv2.waitKey(100) & 0xff == ord('q'):\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def CreateDataSet(video, labels, rate=0.8):\n",
    "    video = np.array(video)\n",
    "    print('video Shape:{}'.format(video.shape))\n",
    "    ids = np.arange(video.shape[0])\n",
    "    rand.shuffle(ids)\n",
    "    trainingCount = int(len(video) * rate)\n",
    "    testCount = len(video)-trainingCount\n",
    "    trainingVideo = []\n",
    "    testVideo = []\n",
    "    trainingLabels = []\n",
    "    testLabels = []\n",
    "    #print(len(video))\n",
    "    #print(range(trainingCount))\n",
    "    #print(range(testCount))\n",
    "    #print(video[0].shape)\n",
    "    for i in range(trainingCount-1) :\n",
    "        trainingVideo.append(video[ids[i]])\n",
    "        trainingLabels.append(labels[ids[i]])\n",
    "    for j in range(testCount-1):\n",
    "        testVideo.append(video[ids[trainingCount+j]])\n",
    "        testLabels.append(labels[ids[trainingCount+j]])\n",
    "    return np.array(trainingVideo), np.array(trainingLabels), np.array(testVideo),  np.array(testLabels)\n",
    "\n",
    "def SplitData(data, rate=0.8):\n",
    "    cuttendDataCount = int(data.shape[0] * rate)\n",
    "    group1 = [] \n",
    "    group2 = []\n",
    "    for i in range(len(data)):\n",
    "        if i < cuttendDataCount:\n",
    "            group2.append(data[i])\n",
    "        else:\n",
    "            group1.append(data[i])\n",
    "            \n",
    "    return np.array(group1), np.array(group2)\n",
    "\n",
    "#imgs, frameCount = LoadVideo('Pig_Identification_Qualification_Train/train/1.mp4')\n",
    "#pImg = Image.fromarray(center_crop(imgs[5]), mode='RGB')\n",
    "#pImg.show()\n",
    "#trainning, testing = SplitData(imgs)\n",
    "#print(trainning[0].shape)\n",
    "#\n",
    "#pImg.show()\n",
    "#pImg = Image.fromarray(testing[0], mode='RGB')\n",
    "#pImg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#pImg = Image.fromarray(testing[0], mode='RGB')\n",
    "#pImg.show()\n",
    "#wd = np.array([0, 2, 3])\n",
    "#np.concatenate((wd, wd))\n",
    "\n",
    "#c = np.concatenate(([None],  [240, 240, 3]))\n",
    "#test = tf.placeholder(dtype=tf.float32, shape=c, name=\"test\")\n",
    "#shape = test.get_shape().as_list()\n",
    "#print(tf.shape(test).as_list())\n",
    "#print(shape)\n",
    "#num_features = np.prod(shape[1:])\n",
    "#print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Spatial_Transfrom (inMap, theta, out_dims=None, **args):\n",
    "    sess=tf.Session()\n",
    "    print(\"inmap{},  shape{}, shapeList{}\".format(inMap, tf.shape(inMap), inMap.get_shape().as_list()))\n",
    "    shape = inMap.get_shape()\n",
    "    B = tf.shape(inMap)[0]\n",
    "    H = shape.as_list()[1]\n",
    "    W = shape.as_list()[2]\n",
    "    C = shape.as_list()[3]\n",
    "    \n",
    "    print('b:{}'.format(B))\n",
    "    #print('w:{}, shape:{}'.format(w, w.get_shape().as_list()))\n",
    "    \n",
    "    #construct theta matrix\n",
    "    theta = tf.reshape(theta, [B, 2, 3])\n",
    "    \n",
    "    if out_dims:\n",
    "        out_H = out_dims[0]\n",
    "        out_W = out_dims[1]\n",
    "        batch_grids = affine_grid_gen(out_H, out_W, theta)\n",
    "    else:\n",
    "        batch_grids = affine_grid_gen(H, W, theta)\n",
    "    \n",
    "    print('batch_grids')\n",
    "    print(batch_grids.get_shape().as_list())\n",
    "    xs = batch_grids[:, 0, :, :]\n",
    "    ys = batch_grids[:, 1, :, :]\n",
    "    \n",
    "    out_samp = bilinear_sampler(inMap, xs, ys)\n",
    "    print('Spatial_Transfrom')\n",
    "    print(inMap.get_shape().as_list())\n",
    "    print(out_samp.get_shape().as_list())\n",
    "    print('\\n')\n",
    "    return out_samp\n",
    "\n",
    "def affine_grid_gen(height, width, theta):\n",
    "    print('affine_grid_gen, height:{}, width:{}, theta:{}'.format(height, width, theta.get_shape().as_list()))\n",
    "    \n",
    "    num_batch = tf.shape(theta)[0]\n",
    "    \n",
    "    #nornalized grid elements\n",
    "    x = tf.linspace(-1.0, 1.0, width)\n",
    "    y = tf.linspace(-1.0, 1.0, height)\n",
    "    x_t, y_t = tf.meshgrid(x, y)\n",
    "    print('affine_grid_gen, x:{}, y:{}, x_t:{}, y_t:{}'.format(x.get_shape().as_list(), y.get_shape().as_list(), \n",
    "                                                               x_t.get_shape().as_list(), y_t.get_shape().as_list()))\n",
    "    \n",
    "    #flatten\n",
    "    x_t_flat = tf.reshape(x_t, [-1])\n",
    "    y_t_flat = tf.reshape(y_t, [-1])\n",
    "    print('affine_grid_gen, x_t_flat:{}, y_t_flat:{}'.format(x_t_flat.get_shape().as_list(), y_t_flat.get_shape().as_list()))\n",
    "    \n",
    "    #generate grid\n",
    "    ones = tf.ones_like(x_t_flat)\n",
    "    print('ones')\n",
    "    print(ones.get_shape().as_list())\n",
    "    sampling_grid = tf.stack([x_t_flat, y_t_flat, ones])\n",
    "    sampling_grid = tf.expand_dims(sampling_grid, axis=0)\n",
    "    sampling_grid = tf.tile(sampling_grid, tf.stack([num_batch, 1, 1]))\n",
    "    \n",
    "    #cast to float32\n",
    "    theta = tf.cast(theta, 'float32')\n",
    "    sampling_grid = tf.cast(sampling_grid, 'float32')\n",
    "    \n",
    "    batch_grids = tf.matmul(theta, sampling_grid)\n",
    "    \n",
    "    batch_grids = tf.reshape(batch_grids, [num_batch, 2, height, width])\n",
    "    \n",
    "    return batch_grids\n",
    "\n",
    "def bilinear_sampler(img, x, y):\n",
    "    print(\"bilinear_sampler x:{}, y:{}\".format(str(x), str(y)))\n",
    "    \n",
    "    B = tf.shape(img)[0]\n",
    "    H = tf.shape(img)[1]\n",
    "    W = tf.shape(img)[2]\n",
    "    C = tf.shape(img)[3]\n",
    "    \n",
    "    max_y = tf.cast(H -1, 'int32')\n",
    "    max_x = tf.cast(W - 1, 'int32')\n",
    "    zero = tf.zeros([], dtype='int32')\n",
    "    \n",
    "    x = tf.cast(x, 'float32')\n",
    "    y = tf.cast(y, 'float32')\n",
    "    \n",
    "    x = 0.5 * ((x + 1.0) * tf.cast(W, 'float32'))\n",
    "    y = 0.5 * ((y + 1.0) * tf.cast(H, 'float32'))\n",
    "    \n",
    "    x0 = tf.cast(tf.floor(x), 'int32')\n",
    "    x1 = x0 + 1\n",
    "    y0 = tf.cast(tf.floor(y), 'int32')\n",
    "    y1 = y0 + 1\n",
    "    \n",
    "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
    "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
    "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
    "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
    "    \n",
    "    Ia = get_pixel_value(img, x0, y0)\n",
    "    Ib = get_pixel_value(img, x0, y1)\n",
    "    Ic = get_pixel_value(img, x1, y0)\n",
    "    Id = get_pixel_value(img, x1, y1)\n",
    "    \n",
    "    x0 = tf.cast(x0, 'float32')\n",
    "    x1 = tf.cast(x1, 'float32')\n",
    "    y0 = tf.cast(y0, 'float32')\n",
    "    y1 = tf.cast(y1, 'float32')\n",
    "\n",
    "    wa = (x1-x) * (y1-y)\n",
    "    wb = (x1-x) * (y-y0)\n",
    "    wc = (x-x0) * (y1-y)\n",
    "    wd = (x-x0) * (y-y0)\n",
    "    \n",
    "    wa = tf.expand_dims(wa, axis=3)\n",
    "    wb = tf.expand_dims(wb, axis=3)\n",
    "    wc = tf.expand_dims(wc, axis=3)\n",
    "    wd = tf.expand_dims(wd, axis=3)\n",
    "    \n",
    "    out = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n",
    "    return out\n",
    "\n",
    "def get_pixel_value(img, x, y):\n",
    "    \"\"\"\n",
    "    Utility function to get pixel value for coordinate\n",
    "    vectors x and y from a  4D tensor image.\n",
    "    Input\n",
    "    -----\n",
    "    - img: tensor of shape (B, H, W, C)\n",
    "    - x: flattened tensor of shape (B*H*W, )\n",
    "    - y: flattened tensor of shape (B*H*W, )\n",
    "    Returns\n",
    "    -------\n",
    "    - output: tensor of shape (B, H, W, C)\n",
    "    \"\"\"\n",
    "    shape = tf.shape(x)\n",
    "    batch_size = shape[0]\n",
    "    height = shape[1]\n",
    "    width = shape[2]\n",
    "\n",
    "    batch_idx = tf.range(0, batch_size)\n",
    "    batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\n",
    "    b = tf.tile(batch_idx, (1, height, width))\n",
    "\n",
    "    indices = tf.stack([b, y, x], 3)\n",
    "\n",
    "    return tf.gather_nd(img, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Define Hyper-paramenters\n",
    "#display_step = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 36\n",
    "epochs = 30\n",
    "#num_epochs = 50\n",
    "num_residual_blocks =22\n",
    "num_residual_blocks_loc = 5\n",
    "train_ema_decay = 0.95\n",
    "require_improvement = 1000\n",
    "keep_probability = 0.8\n",
    "keep_probability_loc1 = 0.8\n",
    "keep_probability_loc2 = 0.8\n",
    "\n",
    "best_validation_accuracy = 0.0\n",
    "\n",
    "H, W, C = 64, 64, 3\n",
    "image_shape = (H, W, C)\n",
    "image_flat_shape = H * W * C\n",
    "num_classes = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, H, W, C], name='x')\n",
    "y = tf.placeholder(tf.int32, [None, num_classes], name='y')\n",
    "#keep_prob_loc1 = tf.placeholder(dtype=tf.float32, name='keep_prob_loc1')\n",
    "#keep_prob_loc2 = tf.placeholder(dtype=tf.float32, name='keep_prob_loc2')\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "#vali_x = tf.placeholder(tf.float32, [None, H, W, C])\n",
    "#vali_y = tf.placeholder(tf.int32, [None, num_classes])\n",
    "#lr_placeholder = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "#phase = tf.placeholder(tf.bool, name='phase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#X_train =  np.array([])\n",
    "#xx = np.ones((64, 64, 3))\n",
    "#np.concatenate(X_train, xx)\n",
    "#LoadVideo('./../Pig_Identification_Qualification_Train/convert/{}.avi'.format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Label_OneHot(count, y_val):\n",
    "    y = []\n",
    "    for i in range(count):\n",
    "        y.append(np.eye(30, dtype='int32')[y_val])\n",
    "    return y\n",
    "\n",
    "#imgs, frameCount = LoadVideo('Pig_Identification_Qualification_Train/convert/{}.avi'.format(i))\n",
    "def LoadData(show_grid=False, sample_rate = 1):\n",
    "    imgSet = []\n",
    "    lablesSet = []\n",
    "    for i in range(30):\n",
    "        imgs, frameCount = LoadVideo('./../Pig_Identification_Qualification_Train/convert/{}.avi'.format(i))\n",
    "        rand.shuffle(imgs)\n",
    "        lables = Label_OneHot(len(imgs), i)\n",
    "        for j in range(int(len(imgs) * sample_rate)):\n",
    "            imgSet.append(imgs[j])\n",
    "            lablesSet.append(lables[j])\n",
    "    \n",
    "    X_train, Y_train, X_test, Y_test = CreateDataSet(imgSet, lablesSet)\n",
    "    \n",
    "    if show_grid:\n",
    "        mask = np.arange(250)\n",
    "        sample = np.reshape(X_train, [-1, H, W, C])[mask]\n",
    "        view_images(sample)\n",
    "        \n",
    "    X_train = np.reshape(X_train, [-1, H, W, C])\n",
    "    X_test = np.reshape(X_test, [-1, H, W, C])\n",
    "    \n",
    "    X_test, X_val = SplitData(X_test)\n",
    "    Y_test, Y_val = SplitData(Y_test)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, X_val, Y_val\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    a = 0\n",
    "    b = 255\n",
    "    return (x-a)/(b-a)\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    num = len(x)\n",
    "    arr = np.zeros((num, 10))\n",
    "    for i, xl in enumerate(x):\n",
    "        arr[i][xl] = 1\n",
    "    return arr\n",
    "\n",
    "def SelectSetFromBatchSet(Batch_size, Data1, Data2):\n",
    "    ids = np.arange(Data1.shape[0])\n",
    "    rand.shuffle(ids)\n",
    "    rand.shuffle(ids)\n",
    "    X_ = []\n",
    "    Y_ = []\n",
    "    for i in range(batch_size):\n",
    "        X_.append(Data1[ids[i]])\n",
    "        Y_.append(Data2[ids[i]])\n",
    "    return X_, Y_\n",
    "\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y:label_batch, keep_prob:1.0, keep_prob_loc:1.0})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: valid_features,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.,\n",
    "                keep_prob_loc1:1.,\n",
    "                keep_prob_loc2:1.\n",
    "    })\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                loss,\n",
    "                valid_acc))\n",
    "    \n",
    "def print_stats2(session, feature_batch, label_batch, val_x, val_y, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    miner_val_x, miner_val_y = SelectSetFromBatchSet(feature_batch.shape[0] * 10, val_x, val_y)\n",
    "    loss = session.run(cost, feed_dict={x:miner_val_x, y:miner_val_y, keep_prob:1.0})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "                    x: miner_val_x,\n",
    "                    y: miner_val_y,\n",
    "                    keep_prob: 1.,\n",
    "    })\n",
    "    \n",
    "    print('Validation Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                    loss,\n",
    "                    valid_acc))\n",
    "\n",
    "\n",
    "\n",
    "def load_Mist_data(root_dir, view_grid=False):\n",
    "    mnist_cluttered = \"mnist_sequence1_sample_5distortions5x5.npz\"\n",
    "    data = np.load(mnist_cluttered)\n",
    "    X_train, y_train = data['x_train'], data['y_train']\n",
    "    X_valid, y_valid = data['x_valid'], data['y_valid']\n",
    "    X_test, y_test = data['x_test'], data['y_test']\n",
    "    \n",
    "    if view_grid:\n",
    "        mask = np.arange(250)\n",
    "        sample = np.reshape(X_train, [-1, H, W])[mask]\n",
    "        view_images(sample)\n",
    "    \n",
    "    X_train = np.reshape(X_train, [-1, H, W, C])\n",
    "    X_test = np.reshape(X_test, [-1, H, W, C])\n",
    "    X_valid = np.reshape(X_valid, [-1, H, W, C])\n",
    "\n",
    "def random_batch(X, Y, batch_size=batch_size):\n",
    "    num_train = len(X)\n",
    "    batch_mask = np.random.choice(num_train, batch_size)\n",
    "    X_batch = X[batch_mask]\n",
    "    y_batch = Y[batch_mask]\n",
    "    \n",
    "    return X_batch, y_batch\n",
    "\n",
    "def validate_acc_loss(sess, loss, accuracy, x_val, y_val):\n",
    "    avg_loss = 0.0\n",
    "    avg_acc = 0.0\n",
    "    total_B, batch_idx = generate_batch_indices(x_val)\n",
    "    #print(\"x_val shape:{}\".format(x_val.shape))\n",
    "    for i in range(total_B):\n",
    "        #create batch\n",
    "        idx = batch_idx[i]\n",
    "        mask = np.arange(idx[0], idx[1])\n",
    "        #print(\"validate_acc_loss mask:{}\".format(mask))\n",
    "        batch_xs, batch_ys = x_val[mask], y_val[mask]\n",
    "        \n",
    "        val_feed_dict = {X:batch_xs, Y:batch_ys, phase:True}\n",
    "        l, a = sess.run([loss, accuracy], feed_dict=val_feed_dict)\n",
    "        \n",
    "        avg_loss += 1 / total_batch\n",
    "        avg_acc += a / total_batch\n",
    "    \n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def test_acc(sess, accuracy, X_test, y_test):\n",
    "    avg_acc = 0.0\n",
    "    total_B, batch_idx = generate_batch_indices(X_test)\n",
    "    \n",
    "    for i in range(total_B):\n",
    "        idx = batch_idx[i]\n",
    "        mask = np.arange(inx[0], idx[1])\n",
    "        batch_xs, batch_ys = X_test[mask], y_test[mask]\n",
    "        \n",
    "        test_feed_dict = {X:batch_xs, y:batch_ys, phase: Fase}\n",
    "        a = sess.run(accuracy, feed_dict=test_feed_dict)\n",
    "        \n",
    "        avg_acc += a / total_B\n",
    "    \n",
    "    return avg_acc\n",
    "\n",
    "def generate_batch_indices(X, batch_size=batch_size):\n",
    "    num_train = len(X)\n",
    "    total_B = int(np.ceil(num_train / float(batch_size)))\n",
    "    batch_indices = [(i * batch_size, min(num_train, (i+1) * batch_size)) for i in range(0, total_B)]\n",
    "    return total_B, batch_indices\n",
    "\n",
    "\n",
    "def train_neural_network(session, optimizer, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    session.run(optimizer, feed_dict={x:feature_batch, y:label_batch,  phase:True})\n",
    "#a, b, c, d = LoadData(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#preprocess_and_data(LoadData, normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "def non_one_hot(x):\n",
    "    return x\n",
    "\n",
    "def one_hot_decode(y):\n",
    "    x = y[:][:]\n",
    "\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "#helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)\n",
    "#valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#im = np.arange(311040).reshape((240, 432, 3))\n",
    "#center_crop(im)\n",
    "#mnist_cluttered = \"./mnist_sequence1_sample_5distortions5x5.npz\"\n",
    "#data = np.load('./mnist_sequence1_sample_5distortion5x5.npz')\n",
    "#X_train = data['X_valid']\n",
    "#pImg = Image.fromarray(X_train)\n",
    "#pImg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    print(logits.get_shape().as_list())\n",
    "    print(labels.get_shape().as_list())\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    return cross_entropy_mean\n",
    "\n",
    "def top_k_error(predictions, labels, k):\n",
    "    batch_size = predictions.get_shape().as_list()[0]\n",
    "    top = tf.nn.in_top_k(predictions, labels, k=1)\n",
    "    #in_top1 = tf.to_float(top)\n",
    "    num_correct = tf.reduce_sum(top)\n",
    "    return (batch_size - num_correct) / float(batch_size)\n",
    "\n",
    "def train_operation(global_step, total_loss):\n",
    "    #tf.summary.scalar('learning_rate', lr_placeholder)\n",
    "    #tf.summary.scalar('train_loss', total_loss)\n",
    "    #tf.summary.scalar('train_top1_error', top1_error)\n",
    "    \n",
    "    #ema = tf.train.ExponentialMovingAverage(train_ema_decay, global_step)\n",
    "    #train_ema_op = ema.apply([total_loss, top1_error])\n",
    "    #tf.summary.scalar('train_top1_error_avg', ema.average(top1_error))\n",
    "    #tf.summary.scalar('train_loss_avg', ema.average(total_loss))\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = opt.minimize(total_loss, global_step=global_step)\n",
    "    return train_op\n",
    "\n",
    "def validation_op(validation_step, top1_error, loss):\n",
    "    ema = tf.train.ExponentialMovingAverage(0.0, validation_step)\n",
    "    ema2 = tf.train.ExponentialMovingAverage(0.95, validation_step)\n",
    "    val_op = tf.group(validation_step.assign_add(1), ema.apply([top1_error, loss]),\n",
    "                          ema2.apply([top1_error, loss]))\n",
    "    top1_error_val = ema.average(top1_error)\n",
    "    top1_error_avg = ema2.average(top1_error)\n",
    "    loss_val = ema.average(loss)\n",
    "    loss_val_avg = ema2.average(loss)\n",
    "    \n",
    "    tf.summary.scalar('val_top1_error', top1_error_val)\n",
    "    tf.summary.scalar('val_top1_error_avg', top1_error_avg)\n",
    "    tf.summary.scalar('val_loss', loss_val)\n",
    "    tf.summary.scalar('val_loss_avg', loss_val_avg)\n",
    "    return val_op\n",
    "\n",
    "\n",
    "def BatchingData(batch_size, X_train, Y_train, trainBatchCount):\n",
    "    \"\"\"\n",
    "    Process batching data\n",
    "    \"\"\"\n",
    "    for i in range(trainBatchCount):\n",
    "        start_t = i*batch_size\n",
    "        X_t_batch = np.cast['float'](X_train[start_t:start_t+batch_size])\n",
    "        Y_t_batch = np.cast['float'](Y_train[start_t:start_t+batch_size])\n",
    "        \n",
    "        yield X_t_batch, Y_t_batch, i\n",
    "\n",
    "        \n",
    "def interpolate_bilinear(src, ri, rf, rc, ti, tf, tc):  \n",
    "    if rf == rc & tc == tf:  \n",
    "        out = src[rc][tc]  \n",
    "    elif rf == rc:  \n",
    "        out = (ti - tf)*src[rf][tc] + (tc - ti)*src[rf][tf]  \n",
    "    elif tf == tc:  \n",
    "        out = (ri - rf)*src[rc][tf] + (rc - ri)*src[rf][tf]  \n",
    "    else:  \n",
    "        inter_r1 = (ti - tf)*src[rf][tc] + (tc - ti)*src[rf][tf]  \n",
    "        inter_r2 = (ti - tf)*src[rc][tc] + (tc - ti)*src[rc][tf]  \n",
    "        out = (ri - rf)*inter_r2 + (rc - ri)*inter_r1  \n",
    "  \n",
    "    return out\n",
    "\n",
    "def load_preprocess_batch(path, batch_size, dataCount):\n",
    "    features, labels = pickle.load(open(path, mode='rb'))\n",
    "    trainBatchCount = int(features.shape[0] / batch_size)\n",
    "    dataCount[0] = trainBatchCount\n",
    "    \n",
    "    return BatchingData(batch_size, features, labels, trainBatchCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "def file_name(file_dir):\n",
    "    L=[]\n",
    "    num=[]\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        #print(files)\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] == '.JPG':\n",
    "                p = os.path.join(root, file)\n",
    "                n = os.path.splitext(file)[0]\n",
    "                yield p, n\n",
    "    #return L, num\n",
    "\n",
    "def loadImgsp():\n",
    "    _x_ = []\n",
    "    paths = []\n",
    "    for path, num in file_name('../Pig_Identification_Qualification_Train/test_set'):\n",
    "        _x = cv2.imread(path)[:,:,::-1]\n",
    "        pImg = Image.fromarray(_x, mode='RGB')\n",
    "        pImg = pImg.resize((64,64))\n",
    "        _x_.append(np.array(pImg))\n",
    "        paths.append(num)\n",
    "        #_y_.append(Label_OneHot(1, rand.randint(1, 30)))\n",
    "    return {'data':np.array(_x_), 'path':paths}\n",
    "\n",
    "def loadImgsAndSave():\n",
    "    _x_ = []\n",
    "    paths = []\n",
    "    for path, num in file_name('../Pig_Identification_Qualification_Train/test_set'):\n",
    "        _x = cv2.imread(path)[:,:,::-1]\n",
    "        pImg = Image.fromarray(_x, mode='RGB')\n",
    "        if _x.shape[0] > _x.shape[1]:\n",
    "            pImg.transpose(Image.ROTATE_90)\n",
    "        pImg = pImg.resize((64,64))\n",
    "        _x_.append(np.array(pImg))\n",
    "        paths.append(num)\n",
    "        #_y_.append(Label_OneHot(1, rand.randint(1, 30)))\n",
    "    result = {'data':np.array(_x_), 'path':paths}\n",
    "    f = open('test_set.p', 'wb')\n",
    "    pickle.dump((result), f)\n",
    "    f.close()\n",
    "    \n",
    "def loadImgs():\n",
    "    f = open('test_set.p', mode='rb')\n",
    "    result = pickle.load(f)\n",
    "    f.close()\n",
    "    return result\n",
    "\n",
    "#path, num = file_name('../Pig_Identification_Qualification_Train/test_set')\n",
    "#print(num)\n",
    "#loadImgsAndSave()\n",
    "xx = loadImgs()\n",
    "print(xx['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(1, 30)\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "ww = rand.randint(1, 30)\n",
    "print(ww)\n",
    "yy = np.array(Label_OneHot(1, ww))\n",
    "print(yy.shape)\n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(30))\n",
    "label_ids = label_binarizer.inverse_transform(yy)\n",
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 30]\n",
      "[None, 30]\n"
     ]
    }
   ],
   "source": [
    "#X_train, Y_train, X_test, Y_test, X_val, Y_val = LoadData(True, sample_rate = 0.5)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "validation_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "#num_train = X_train.shape[0]\n",
    "#num_val = X_val.shape[0]\n",
    "\n",
    "#logit_loc2 = inference(x, num_residual_blocks_loc, dropout=0.92, reuse=False, num_class=6)\n",
    "#h_trans2 = Spatial_Transfrom(x, logit_loc2)\n",
    "logits = inference(x, num_residual_blocks, dropout=keep_prob, reuse=False, num_class=num_classes)\n",
    "logits = tf.identity(logits, name='logits')\n",
    "#vali_logits = inference(vali_x, num_residual_blocks,   reuse=True)\n",
    "\n",
    "regu_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "train_loss = loss(logits, y)\n",
    "\n",
    "full_loss = tf.add_n([train_loss] + regu_losses)\n",
    "\n",
    "#predictions = tf.nn.softmax(logits)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "#print(predictions.get_shape().as_list())\n",
    "#train_top1_error = top_k_error(predictions, vali_y, 1)\n",
    "\n",
    "#vali_loss = loss(vali_logits, vali_y)\n",
    "#vali_predictions = tf.nn.softmax(vali_logits)\n",
    "#vali_top1_error = top_k_error(vali_predictions, vali_y, 1)\n",
    "\n",
    "train_op = train_operation(global_step, full_loss)\n",
    "#val_op  = validation_op(validation_step, vali_top1_error, vali_loss)\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "summary_op = tf.summary.merge_all()\n",
    "#summary_writer = tf.train.SummaryWriter('/notran_small_logs', sess.graph)\n",
    "#init = tf.initialize_all_variables()\n",
    "\n",
    "X_val, Y_val = pickle.load(open('./preprocess_batch_val.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def saveOutput(sess, number):    \n",
    "    path__ = []\n",
    "    ww = rand.randint(1, 25)\n",
    "    batch_count = int(xx['data'].shape[0]/batch_size)\n",
    "    text_=''\n",
    "    __text = ''\n",
    "    _text_ = ''\n",
    "    for _x_, _y_, batch_i in BatchingData(batch_size, xx['data'], np.array(Label_OneHot(3000, ww)), batch_count):\n",
    "        __x = normalize(_x_)\n",
    "        predictions  = sess.run(logits, feed_dict={x:__x, keep_prob:1.})\n",
    "        random_test_predictions  = sess.run(tf.nn.top_k(tf.nn.softmax(logits), 30), feed_dict={x:__x,  keep_prob:1.})\n",
    "        \n",
    "        label_binarizer = LabelBinarizer()\n",
    "        label_binarizer.fit(range(30))\n",
    "        label_ids = label_binarizer.inverse_transform(predictions)\n",
    "        \n",
    "        print('label_ids shape{},test_predictions shape {}'.format(label_ids.shape, random_test_predictions.values.shape))\n",
    "        \n",
    "        for idx in range(batch_size):\n",
    "            _text_ += '{}, {}, {}, {}\\n'.format(xx['path'][idx + (batch_i * batch_size)], \n",
    "                                           random_test_predictions.indices[idx][0], random_test_predictions.values[idx][0], label_ids[idx])\n",
    "            \n",
    "            for idxx in range(30):\n",
    "                __text += '{}, {}, {:.15f}\\n'.format(xx['path'][idx + (batch_i * batch_size)], \n",
    "                                           random_test_predictions.indices[idx][idxx]+1, random_test_predictions.values[idx][idxx])\n",
    "            \n",
    "            if idx % 5 == 0:\n",
    "                text_ += 'filename : {}, Label: {}, Prediction: {}\\n'.format(xx['path'][idx + (batch_i * batch_size)], \n",
    "                                           label_ids[idx], random_test_predictions.values[idx][:3])\n",
    "                #print(text_)\n",
    "            #text_ += '\\n'\n",
    "\n",
    "        #print(text_)\n",
    "        \n",
    "       \n",
    "        path__ = []\n",
    "    \n",
    "    #print(__text[0:200])\n",
    "    f2 = open('./output3_{}.csv'.format(number), 'w')\n",
    "    f2.write(__text)\n",
    "    f2.close()\n",
    "    f1=open('./output3_log_{}.log'.format(number), 'w')\n",
    "    f1.write(text_)\n",
    "    f1.close()\n",
    "    f3 = open('./output3_d_{}.csv'.format(number), 'w')\n",
    "    f3.write(_text_)\n",
    "    f3.close()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"./tran_small3\",sess.graph)  \n",
    "   # trainBatchCount = int(X_train.shape[0] / batch_size)\n",
    "   # valBatchCount = int(X_val.shape[0] / batch_size)\n",
    "  #  print(\"Training on {} samples, validating on{}\".format(len(X_train), len(X_val)))\n",
    "    #fig = plot.figure()\n",
    "    #view_images(X_train[0])\n",
    "    # Training cycle\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        epoch = i\n",
    "        start_time = time.time()\n",
    "        batch_i = 0\n",
    "        dataCount = [0]\n",
    "        for x_t, y_t, batch_i in load_preprocess_batch('preprocess_batch_tranning.p', batch_size, dataCount):\n",
    "            if batch_i == epoch:\n",
    "                view_images(x_t)\n",
    "                print(y_t)\n",
    "                \n",
    "            _, train_loss_value, = sess.run([train_op, full_loss], {x:x_t, y:y_t, \n",
    "                                                                    keep_prob:keep_probability})\n",
    "            #thetas = sess.run(h_trans2, feed_dict={x:x_t, keep_prob_loc1:1, keep_prob_loc2:1})\n",
    "            if batch_i % int(dataCount[0] / 100) == 0:\n",
    "                print('Trainning batch {}/{} in epoch {}, local loss is {:.4f}'.format(batch_i, dataCount[0], epoch+1, train_loss_value))\n",
    "            \n",
    "            if batch_i >= dataCount[0]:\n",
    "                summary_str = sess.run(summary_op, feed_dict={x:x_t, y:y_t, \n",
    "                                                                    keep_prob:keep_probability})\n",
    "                writer.add_summary(summary_str, batch_i)\n",
    "            \n",
    "            \n",
    "        #thetas = sess.run(h_trans, feed_dict={x:batch_features, phase:True})\n",
    "        #thetas = thetas[0:9].squeeze()\n",
    "        #thetas = sess.run(h_trans2, feed_dict={x:x_t,keep_prob:1. })\n",
    "        #view_images(thetas[0:9])\n",
    "        #rint ('Train top1 error = '+ train_error_value)\n",
    "        #print ('Validation top1 error = {:.4f}'.format(validation_error_value))\n",
    "        #print ('Validation loss = '+validation_loss_value)\n",
    "        print('Epoch {:>2}'.format(epoch+1))\n",
    "        \n",
    "        print_stats2(sess, x_t, y_t, X_val, Y_val, full_loss, accuracy)\n",
    "        \n",
    "        try:\n",
    "            if i == 4:\n",
    "                saveOutput(sess, 5)\n",
    "\n",
    "            if i == 9:\n",
    "                saveOutput(sess, 10)\n",
    "\n",
    "            if i == 14:\n",
    "                saveOutput(sess, 15)\n",
    "\n",
    "            if i == 24:\n",
    "                saveOutput(sess, 15)\n",
    "        except Exception as e3:\n",
    "            \n",
    "            pass\n",
    "        \n",
    "        \n",
    "        #thetas = thetas[0:9].squeeze()\n",
    "        #plot.clf()\n",
    "        #view_images(thetas[0:9])\n",
    "        #fig.canvas.draw()\n",
    "        #plot.savefig('/convert240/plotting_figure/epoch_'+str(epoch)+'_thetas.png', bbox_inches='tight')\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        examples_per_sec = batch_size / duration\n",
    "        \n",
    "        format_str = ('epoch Summary: {}: loss = {:.4f} ({:.1f} examples/sec; {:.3f} ' 'sec/batch)')\n",
    "        print (format_str.format(datetime.now(), train_loss_value, examples_per_sec, duration))\n",
    "        print ('----------------------------')\n",
    "        \n",
    "        #plot.clf()\n",
    "        #print('Print theta')\n",
    "        #for j in range(9):\n",
    "        #    plot.subplot(3,3, j+1)\n",
    "        #    plot.imshow(thetas[j], cmap='gray')\n",
    "        #    plot.axis('off')\n",
    "        \n",
    "    save_model_path = './Pigs_identity_Tran_small3'\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)\n",
    "    \n",
    "    test_total_acc = 0\n",
    "    \n",
    "    for feature, labels, batch_i in  load_preprocess_batch('./preprocess_batch_testing.p', batch_size, dataCount):\n",
    "        test_total_acc += sess.run(accuracy, feed_dict={x:feature, y:labels, keep_prob:1.})\n",
    "        print(\"the batch at {}, local acc is {}\".format(batch_i, test_total_acc))\n",
    "        \n",
    "    print(\"Testing Accuracy: {}\\n\".format(test_total_acc/batch_i))\n",
    "    saveOutput(sess, 'end')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#print( rand.randint(1, 30))\n",
    "#print(Label_OneHot(30, rand.randint(1, 30)))\n",
    "#cap = cv2.VideoCapture('../Pig_Identification_Qualification_Train/test_set/10.jpg')\n",
    "tempx = cv2.imread('../Pig_Identification_Qualification_Train/test_set/10.jpg', 1)[:,:,::-1]\n",
    "pImg = Image.fromarray(tempx, mode='RGB')\n",
    "pImg = pImg.resize((64,64))\n",
    "oo2 = np.array([np.array(pImg)])\n",
    "tempx = np.array([tempx])\n",
    "print(oo2.shape)\n",
    "print(tempx.shape)\n",
    "#print(tempx[[:,:,::-1]])\n",
    "view_images(oo2)\n",
    "view_images(tempx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Pigs_identity_Tran_small3\n",
      "<class 'tensorflow.python.framework.ops.GraphKeys'>\n",
      "loaded_xTensor(\"x:0\", shape=(?, 64, 64, 3), dtype=float32) shape:(?, 64, 64, 3)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n",
      "label_ids shape(36,),test_predictions shape (36, 30)\n"
     ]
    }
   ],
   "source": [
    "#X_train, Y_train, X_test, Y_test, X_val, Y_val = LoadData(True, sample_rate = 0.2)\n",
    "\n",
    "\n",
    "save_model_path = './Pigs_identity_Tran_small3'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "loaded_graph = tf.get_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    loader = tf.train.import_meta_graph(save_model_path+'.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "    \n",
    "    print(tf.GraphKeys)\n",
    "    \n",
    "    loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "    loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "    #loaded_keep_prob_loc1 = loaded_graph.get_tensor_by_name('keep_prob_loc1:0')\n",
    "    #loaded_keep_prob_loc2 = loaded_graph.get_tensor_by_name('keep_prob_loc2:0')\n",
    "    loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "    loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "    \n",
    "    print('loaded_x{} shape:{}'.format(loaded_x, loaded_x.get_shape()))\n",
    "    \n",
    "    #a = Label_OneHot(30, rand.randint(1, 30))\n",
    "    #b = Label_OneHot(30, rand.randint(1, 30))\n",
    "    #tempx = np.random.rand(2, 64, 64, 3)\n",
    "    #tempy = np.array(Label_OneHot(2, rand.randint(1, 30)))\n",
    "    \n",
    "    #test_total_acc = sess.run(loaded_acc, feed_dict={loaded_x:tempx, loaded_y:tempy, \n",
    "                                                        #  loaded_keep_prob_loc1:1., loaded_keep_prob_loc2:1., loaded_keep_prob:1.})\n",
    "    #print('Test Run:{}'.format(test_total_acc))\n",
    "    test_total_acc = 0\n",
    "    #testBatchCount = int(X_test.shape[0] / batch_size)\n",
    "    datacount = [0]\n",
    "    #for feature, labels, batch_i in  load_preprocess_batch('./preprocess_batch_testing.p', batch_size, datacount):\n",
    "        #print(feature)\n",
    "        #print(batch_i)\n",
    "        #test_total_acc += sess.run(loaded_acc, feed_dict={loaded_x:feature, loaded_y:labels, loaded_keep_prob:1.})\n",
    "        #print(\"the batch at {}, local acc is {}\".format(batch_i, test_total_acc))\n",
    "        \n",
    "    #print(\"Testing Accuracy: {}\\n\".format(test_total_acc/datacount[0]))\n",
    "    \n",
    "    saveOutput(sess, 'end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(__text[0:200])\n",
    "f2 = open('./output1.csv', 'w')\n",
    "f2.write(__text)\n",
    "f2.close()\n",
    "f1=open('./output_log.log', 'w')\n",
    "f1.write(text_)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2988,)\n",
      "0.344289085669\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def logloss (predictions):\n",
    "    totalLoss = 0\n",
    "    logPre = np.log(predictions)\n",
    "    for n in range(logPre.shape[0]):\n",
    "        totalLoss += logPre[n]\n",
    "    logLoss = - 1 / len(predictions) * totalLoss\n",
    "    print(logLoss)\n",
    "    \n",
    "nump = []\n",
    "with open('./output3_d_end.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        nump.append(float(row[2]))\n",
    "nump = np.array(nump)\n",
    "print(nump.shape)\n",
    "logloss(nump)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nump = []\n",
    "with open('./output1.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        nump.append(row)\n",
    "\n",
    "nump = np.cast['float'](np.array(nump))\n",
    "kkk = []\n",
    "for jjj in range(int(nump.shape[0] / 30)):\n",
    "    lll = nump[jjj * 30:jjj * 30 + 30]\n",
    "    #print(lll)\n",
    "    lll.sort(axis=0)\n",
    "    for eles in range(lll.shape[0]):\n",
    "        kkk.append(lll[eles])\n",
    "kkk = np.array(kkk)\n",
    "print(kkk[:35])\n",
    "\n",
    "\n",
    "#nump.sort(axis=0, )\n",
    "#print(nump[:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(kkk.shape)\n",
    "with open('./output1_new.csv', 'w') as w:\n",
    "    writer = csv.writer(w)\n",
    "    for jjs in range(kkk.shape[0]):\n",
    "        writer.writerow([str(int(kkk[jjs][0])), str(int(kkk[jjs][1])), '{:.15f}'.format(kkk[jjs][2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, X_val, Y_val = LoadData(True, sample_rate = 0.1)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "validation_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "num_train = X_train.shape[0]\n",
    "num_val = X_val.shape[0]\n",
    "\n",
    "logit_loc1 = inference(x, num_residual_blocks_loc, dropout=keep_prob_loc1, reuse=False, num_class=6)\n",
    "h_trans1 = Spatial_Transfrom(x, logit_loc1)\n",
    "logit_loc2 = inference(h_trans1, num_residual_blocks_loc, dropout=keep_prob_loc2, reuse=False, num_class=6)\n",
    "h_trans2 = Spatial_Transfrom(h_trans1, logit_loc1, out_dims=[H, H])\n",
    "logits = inference(h_trans2, num_residual_blocks, dropout=keep_prob, reuse=False, num_class=num_classes)\n",
    "logits = tf.identity(logits, name='logits')\n",
    "#vali_logits = inference(vali_x, num_residual_blocks,   reuse=True)\n",
    "\n",
    "regu_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "train_loss = loss(logits, y)\n",
    "\n",
    "full_loss = tf.add_n([train_loss] + regu_losses)\n",
    "\n",
    "#predictions = tf.nn.softmax(logits)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "#print(predictions.get_shape().as_list())\n",
    "#train_top1_error = top_k_error(predictions, vali_y, 1)\n",
    "\n",
    "#vali_loss = loss(vali_logits, vali_y)\n",
    "#vali_predictions = tf.nn.softmax(vali_logits)\n",
    "#vali_top1_error = top_k_error(vali_predictions, vali_y, 1)\n",
    "\n",
    "train_op = train_operation(global_step, full_loss)\n",
    "#val_op  = validation_op(validation_step, vali_top1_error, vali_loss)\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "summary_op = tf.summary.merge_all()\n",
    "#init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    trainBatchCount = int(X_train.shape[0] / batch_size)\n",
    "    valBatchCount = int(X_val.shape[0] / batch_size)\n",
    "    print(\"Training on {} samples, validating on{}\".format(len(X_train), len(X_val)))\n",
    "    fig = plot.figure()\n",
    "    view_images(X_train[0])\n",
    "    # Training cycle\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        epoch = i\n",
    "        start_time = time.time()\n",
    "        batch_i = 0\n",
    "        for x_t, y_t, batch_i in  BatchingData(batch_size, X_train, Y_train, trainBatchCount):\n",
    "            _, train_loss_value, = sess.run([train_op, full_loss], {x:x_t, y:y_t, \n",
    "                                                                    keep_prob:keep_probability, \n",
    "                                                                    keep_prob_loc1:keep_probability_loc1,\n",
    "                                                                    keep_prob_loc2:keep_probability_loc2})\n",
    "            thetas = sess.run(h_trans2, feed_dict={x:x_t, keep_prob_loc1:1, keep_prob_loc2:1})\n",
    "            \n",
    "        #thetas = sess.run(h_trans, feed_dict={x:batch_features, phase:True})\n",
    "        #thetas = thetas[0:9].squeeze()\n",
    "        \n",
    "        #rint ('Train top1 error = '+ train_error_value)\n",
    "        #print ('Validation top1 error = {:.4f}'.format(validation_error_value))\n",
    "        #print ('Validation loss = '+validation_loss_value)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch, batch_i), end='')\n",
    "        print_stats2(sess, x_t, y_t, X_val, Y_val, full_loss, accuracy)\n",
    "        \n",
    "        #thetas = thetas[0:9].squeeze()\n",
    "        plot.clf()\n",
    "        view_images(thetas)\n",
    "        for j in range(9):\n",
    "            plot.subplot(3,3, j+1)\n",
    "            plot.imshow(thetas[j], cmap='gray')\n",
    "            plot.axis('off')\n",
    "        fig.canvas.draw()\n",
    "        plot.savefig('./plotting_figure/epoch_'+str(epoch)+'_thetas.png', bbox_inches='tight')\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        examples_per_sec = batch_size / duration\n",
    "        \n",
    "        format_str = ('epoch Summary: {}: loss = {:.4f} ({:.1f} examples/sec; {:.3f} ' 'sec/batch)')\n",
    "        print (format_str.format(datetime.now(), train_loss_value, examples_per_sec, duration))\n",
    "        print ('----------------------------')\n",
    "        \n",
    "        #plot.clf()\n",
    "        #print('Print theta')\n",
    "        #for j in range(9):\n",
    "        #    plot.subplot(3,3, j+1)\n",
    "        #    plot.imshow(thetas[j], cmap='gray')\n",
    "        #    plot.axis('off')\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef init_weights(name, shape):\\n    init = tf.contrib.layers.variance_scaling_initializer()\\n    w = tf.get_variable(name, shape, tf.float32, init)\\n    \\n    return w\\n\\ndef init_bias(name, shape, trans=False):\\n    init = tf.zeros_initializer\\n    b = tf.get_variable(name, shape, tf.float32, init)\\n    if trans:\\n        x = np.array([[1., 0, 0], [0, 1., 0]])\\n        x = x.astype('float32').flatten()\\n        b = tf.Variable(initial_value=x)\\n    return b\\n\\ndef Conv2D(input_tensor, input_shape, filter_size, num_filters, strides=1, name=None):\\n    \\n    #conv net helper\\n    \\n    shape = [filter_size, filter_size, input_shape, num_filters]\\n    w = init_weights(name=name+'_w', shape=shape)\\n    b = init_bias(name=name+'_b', shape=shape[-1])\\n    \\n    conv = tf.nn.conv2d(input_tensor, w, strides=[1, strides, strides, 1], padding='SAME', name=name)\\n    conv = tf.nn.bias_add(conv, b)\\n    return conv\\n    \\ndef MaxPolling2D(input_tensor, k=2, use_relu=False, name=None):\\n    pool = tf.nn.max_pool(input_tensor, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME', name=name)\\n    \\n    if use_relu:\\n        pool = tf.nn.relu(pool)\\n    \\n    return pool\\n\\ndef Flatten(layer):\\n    layer_shape = layer.get_shape()\\n    num_features = layer_shape[1:].num_elements()\\n    layer_flat = tf.reshape(layer, [-1, num_features])\\n    return layer_flat, number_features\\n\\ndef BatchNormalization(input_tensor, phase, use_relu=False, name=None):\\n    normed = tf.contrib.layers.batch_norm(input_tensor, center=True, is_trainning=phase, scope=name)\\n    if use_relu:\\n        normed = tf.nn.relu(normed)\\n    return normed\\n\\ndef Dense(input_tensor, num_inputs, num_outputs, use_relu=True, trans=False, name=None):\\n    shape = [num_inputs, num_outputs]\\n    \\n    w = init_weights(name=name+'_w', shape=shape)\\n    b = init_bias(name=name+'_b', shape=shape[-1], trans=trans)\\n    \\n    fc = tf.matmul(input_tensor, w, name=name) + b\\n    \\n    if use_relu:\\n        fc = tf.nn.relu(fc)\\n        \\n    return fc\\n\\ndef theta_bias(name):\\n    with tf.variable_scope(name):\\n        x = np.array([1., 0, 0], [0, 1., 0])\\n        x = x.astype('float32').flatten()\\n        return tf.Variable(initial_value=x)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def init_weights(name, shape):\n",
    "    init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    w = tf.get_variable(name, shape, tf.float32, init)\n",
    "    \n",
    "    return w\n",
    "\n",
    "def init_bias(name, shape, trans=False):\n",
    "    init = tf.zeros_initializer\n",
    "    b = tf.get_variable(name, shape, tf.float32, init)\n",
    "    if trans:\n",
    "        x = np.array([[1., 0, 0], [0, 1., 0]])\n",
    "        x = x.astype('float32').flatten()\n",
    "        b = tf.Variable(initial_value=x)\n",
    "    return b\n",
    "\n",
    "def Conv2D(input_tensor, input_shape, filter_size, num_filters, strides=1, name=None):\n",
    "    \n",
    "    #conv net helper\n",
    "    \n",
    "    shape = [filter_size, filter_size, input_shape, num_filters]\n",
    "    w = init_weights(name=name+'_w', shape=shape)\n",
    "    b = init_bias(name=name+'_b', shape=shape[-1])\n",
    "    \n",
    "    conv = tf.nn.conv2d(input_tensor, w, strides=[1, strides, strides, 1], padding='SAME', name=name)\n",
    "    conv = tf.nn.bias_add(conv, b)\n",
    "    return conv\n",
    "    \n",
    "def MaxPolling2D(input_tensor, k=2, use_relu=False, name=None):\n",
    "    pool = tf.nn.max_pool(input_tensor, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME', name=name)\n",
    "    \n",
    "    if use_relu:\n",
    "        pool = tf.nn.relu(pool)\n",
    "    \n",
    "    return pool\n",
    "\n",
    "def Flatten(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:].num_elements()\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    return layer_flat, number_features\n",
    "\n",
    "def BatchNormalization(input_tensor, phase, use_relu=False, name=None):\n",
    "    normed = tf.contrib.layers.batch_norm(input_tensor, center=True, is_trainning=phase, scope=name)\n",
    "    if use_relu:\n",
    "        normed = tf.nn.relu(normed)\n",
    "    return normed\n",
    "\n",
    "def Dense(input_tensor, num_inputs, num_outputs, use_relu=True, trans=False, name=None):\n",
    "    shape = [num_inputs, num_outputs]\n",
    "    \n",
    "    w = init_weights(name=name+'_w', shape=shape)\n",
    "    b = init_bias(name=name+'_b', shape=shape[-1], trans=trans)\n",
    "    \n",
    "    fc = tf.matmul(input_tensor, w, name=name) + b\n",
    "    \n",
    "    if use_relu:\n",
    "        fc = tf.nn.relu(fc)\n",
    "        \n",
    "    return fc\n",
    "\n",
    "def theta_bias(name):\n",
    "    with tf.variable_scope(name):\n",
    "        x = np.array([1., 0, 0], [0, 1., 0])\n",
    "        x = x.astype('float32').flatten()\n",
    "        return tf.Variable(initial_value=x)\n",
    "\"\"\"\n",
    "#w = init_weights('testW', image_shape)\n",
    "#b = init_bias('testB', image_shape)\n",
    "#convTest = Conv2D(X, 1, 5, 32, name=\"convTest\")\n",
    "#poolTest = MaxPolling2D(X, use_relu=True, name='poolTest')\n",
    "#flat = Flatten(X)\n",
    "#dense = Dense(X, 2048, 512, use_relu=True, trans=True, name='FCTest')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1st test\n",
    "# \n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "validation_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "conv0 = conv_bn_relu_layer(x, [3, 3, 3, 16], 1)\n",
    "activation_summary(conv0)\n",
    "conv1 = residual_block(conv0, 16, keep_prob=1, first_block=True)\n",
    "activation_summary(conv1)\n",
    "bn_layer_local_1 = batch_normalization_layer(conv1)\n",
    "conv2 = residual_block(bn_layer_local_1,  32, keep_prob=1)\n",
    "activation_summary(conv2)\n",
    "bn_layer_local_2 = batch_normalization_layer(conv2)\n",
    "relu_local = tf.nn.relu(bn_layer_local_2)\n",
    "loc_pool = tf.reduce_mean(relu_local, [1, 2])\n",
    "output_loc = output_layer(loc_pool, 6)\n",
    "\n",
    "h_trans = Spatial_Transfrom(x, output_loc)\n",
    "print(h_trans.get_shape().as_list())\n",
    "logits = inference(h_trans, num_residual_blocks, dropout=keep_prob, reuse=False)\n",
    "#vali_logits = inference(vali_x, num_residual_blocks,   reuse=True)\n",
    "\n",
    "regu_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "train_loss = loss(logits, y)\n",
    "\n",
    "full_loss = tf.add_n([train_loss] + regu_losses)\n",
    "\n",
    "#predictions = tf.nn.softmax(logits)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "#print(predictions.get_shape().as_list())\n",
    "#train_top1_error = top_k_error(predictions, vali_y, 1)\n",
    "\n",
    "#vali_loss = loss(vali_logits, vali_y)\n",
    "#vali_predictions = tf.nn.softmax(vali_logits)\n",
    "#vali_top1_error = top_k_error(vali_predictions, vali_y, 1)\n",
    "\n",
    "train_op = train_operation(global_step, full_loss)\n",
    "#val_op  = validation_op(validation_step, vali_top1_error, vali_loss)\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "summary_op = tf.summary.merge_all()\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_error_list = []\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        fig = plot.figure()\n",
    "        start_time = time.time()\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            #rain_neural_network(sess, optimizer, batch_features, batch_labels)\n",
    "            \n",
    "            \n",
    "            \n",
    "            _, train_loss_value, = sess.run([train_op, full_loss], {x:batch_features, y:batch_labels, keep_prob:keep_probability})\n",
    "            \n",
    "            thetas = sess.run(h_trans, feed_dict={x:batch_features})\n",
    "            thetas = thetas[0:9].squeeze()\n",
    "            #thetas = sess.run(h_trans, feed_dict={x:batch_features, phase:True})\n",
    "            #thetas = thetas[0:9].squeeze()\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        examples_per_sec = batch_size / duration\n",
    "        \n",
    "        \n",
    "        plot.clf()\n",
    "        for j in range(9):\n",
    "            plot.subplot(3,3, j+1)\n",
    "            plot.imshow(thetas[j], cmap='gray')\n",
    "            plot.axis('off')\n",
    "        fig.canvas.draw()\n",
    "        plot.savefig('./plotting_figure/epoch_'+str(epoch)+'_thetas.png', bbox_inches='tight')\n",
    "        \n",
    "        format_str = ('{}: loss = {:.4f} ({:.1f} examples/sec; {:.3f} ' 'sec/batch)')\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print (format_str.format(time.time(), train_loss_value, examples_per_sec, duration))\n",
    "        #rint ('Train top1 error = '+ train_error_value)\n",
    "        #print ('Validation top1 error = {:.4f}'.format(validation_error_value))\n",
    "        #print ('Validation loss = '+validation_loss_value)\n",
    "        print_stats(sess, batch_features, batch_labels, full_loss, accuracy)\n",
    "        print ('----------------------------')\n",
    "        \n",
    "        #plot.clf()\n",
    "        #print('Print theta')\n",
    "        #for j in range(9):\n",
    "        #    plot.subplot(3,3, j+1)\n",
    "        #    plot.imshow(thetas[j], cmap='gray')\n",
    "        #    plot.axis('off')\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2nd Test\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "validation_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "logit_loc = inference(x, 4, dropout=0.7, reuse=False, num_class=6)\n",
    "h_trans = Spatial_Transfrom(x, logit_loc)\n",
    "logits = inference(h_trans, num_residual_blocks, dropout=keep_prob, reuse=False, num_class=10)\n",
    "#vali_logits = inference(vali_x, num_residual_blocks,   reuse=True)\n",
    "\n",
    "regu_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "train_loss = loss(logits, y)\n",
    "\n",
    "full_loss = tf.add_n([train_loss] + regu_losses)\n",
    "\n",
    "#predictions = tf.nn.softmax(logits)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "#print(predictions.get_shape().as_list())\n",
    "#train_top1_error = top_k_error(predictions, vali_y, 1)\n",
    "\n",
    "#vali_loss = loss(vali_logits, vali_y)\n",
    "#vali_predictions = tf.nn.softmax(vali_logits)\n",
    "#vali_top1_error = top_k_error(vali_predictions, vali_y, 1)\n",
    "\n",
    "train_op = train_operation(global_step, full_loss)\n",
    "#val_op  = validation_op(validation_step, vali_top1_error, vali_loss)\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "summary_op = tf.summary.merge_all()\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_error_list = []\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        fig = plot.figure()\n",
    "        start_time = time.time()\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            #rain_neural_network(sess, optimizer, batch_features, batch_labels)\n",
    "            \n",
    "            \n",
    "            \n",
    "            _, train_loss_value, = sess.run([train_op, full_loss], {x:batch_features, y:batch_labels, keep_prob:keep_probability})\n",
    "            \n",
    "            thetas = sess.run(h_trans, feed_dict={x:batch_features})\n",
    "            thetas = thetas[0:9].squeeze()\n",
    "            #thetas = sess.run(h_trans, feed_dict={x:batch_features, phase:True})\n",
    "            #thetas = thetas[0:9].squeeze()\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        examples_per_sec = batch_size / duration\n",
    "        \n",
    "        \n",
    "        plot.clf()\n",
    "        for j in range(9):\n",
    "            plot.subplot(3,3, j+1)\n",
    "            plot.imshow(thetas[j], cmap='gray')\n",
    "            plot.axis('off')\n",
    "        fig.canvas.draw()\n",
    "        plot.savefig('./plotting_figure/epoch_'+str(epoch)+'_thetas.png', bbox_inches='tight')\n",
    "        \n",
    "        format_str = ('{}: loss = {:.4f} ({:.1f} examples/sec; {:.3f} ' 'sec/batch)')\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print (format_str.format(time.time(), train_loss_value, examples_per_sec, duration))\n",
    "        #rint ('Train top1 error = '+ train_error_value)\n",
    "        #print ('Validation top1 error = {:.4f}'.format(validation_error_value))\n",
    "        #print ('Validation loss = '+validation_loss_value)\n",
    "        print_stats(sess, batch_features, batch_labels, full_loss, accuracy)\n",
    "        print ('----------------------------')\n",
    "        \n",
    "        #plot.clf()\n",
    "        #print('Print theta')\n",
    "        #for j in range(9):\n",
    "        #    plot.subplot(3,3, j+1)\n",
    "        #    plot.imshow(thetas[j], cmap='gray')\n",
    "        #    plot.axis('off')\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
